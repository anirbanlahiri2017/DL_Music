{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srd/Desktop/music/venv_python3.7/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/srd/Desktop/music/venv_python3.7/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import librosa\n",
    "import itertools\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.layers import Conv2D,concatenate,Lambda,Bidirectional,GRU\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility purposes\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "> Helper functions to assist the process to read songs, split then and return an array of spectrograms/melspectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@description: Method to split a song into multiple songs using overlapping windows\n",
    "\"\"\"\n",
    "def splitsongs(X, y, window = 0.05, overlap = 0.5):\n",
    "    # Empty lists to hold our results\n",
    "    temp_X = []\n",
    "    temp_y = []\n",
    "\n",
    "    # Get the input song array size\n",
    "    xshape = X.shape[0]\n",
    "    chunk = int(xshape*window)\n",
    "    offset = int(chunk*(1.-overlap))\n",
    "    \n",
    "    # Split the song and create new ones on windows\n",
    "    spsong = [X[i:i+chunk] for i in range(0, xshape - chunk + offset, offset)]\n",
    "    for s in spsong:\n",
    "        if s.shape[0] != chunk:\n",
    "            continue\n",
    "\n",
    "        temp_X.append(s)\n",
    "        temp_y.append(y)\n",
    "\n",
    "    return np.array(temp_X), np.array(temp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@description: Method to convert a list of songs to a np array of melspectrograms\n",
    "\"\"\"\n",
    "def to_melspectrogram(songs, n_fft=1024, hop_length=256):\n",
    "    # Transformation function\n",
    "    melspec = lambda x: librosa.feature.melspectrogram(x, n_fft=n_fft,\n",
    "        hop_length=hop_length, n_mels=128)[:,:,np.newaxis]\n",
    "\n",
    "    # map transformation of input songs to melspectrogram using log-scale\n",
    "    tsongs = map(melspec, songs)\n",
    "    # np.array([librosa.power_to_db(s, ref=np.max) for s in list(tsongs)])\n",
    "    return np.array(list(tsongs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_convert(X, y):\n",
    "    arr_specs, arr_genres = [], []\n",
    "    \n",
    "    # Convert to spectrograms and split into small windows\n",
    "    for fn, genre in zip(X, y):\n",
    "        signal, sr = librosa.load(fn)\n",
    "        signal = signal[:song_samples]\n",
    "\n",
    "        # Convert to dataset of spectograms/melspectograms\n",
    "        signals, y = splitsongs(signal, genre)\n",
    "\n",
    "        # Convert to \"spec\" representation\n",
    "        specs = to_melspectrogram(signals)\n",
    "\n",
    "        # Save files\n",
    "        arr_genres.extend(y)\n",
    "        arr_specs.extend(specs)\n",
    "    \n",
    "    return np.array(arr_specs), to_categorical(arr_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(src_dir, genres, song_samples):    \n",
    "    # Empty array of dicts with the processed features from all files\n",
    "    arr_fn = []\n",
    "    arr_genres = []\n",
    "\n",
    "    # Get file list from the folders\n",
    "    for x,_ in genres.items():\n",
    "        folder = src_dir + x\n",
    "        for root, subdirs, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                file_name = folder + \"/\" + file\n",
    "\n",
    "                # Save the file name and the genre\n",
    "                arr_fn.append(file_name)\n",
    "                arr_genres.append(genres[x])\n",
    "    \n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        arr_fn, arr_genres, test_size=0.3, random_state=42, stratify=arr_genres\n",
    "    )\n",
    "    \n",
    "    # Split into small segments and convert to spectrogram\n",
    "    X_train, y_train = split_convert(X_train, y_train)\n",
    "    X_test, y_test = split_convert(X_test, y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gtzan_dir = '../data/genres/'\n",
    "song_samples = 660000\n",
    "genres = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4, \n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}\n",
    "\n",
    "# Read the data\n",
    "X_train, X_test, y_train, y_test = read_data(gtzan_dir, genres, song_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27300, 128, 129, 1) (11700, 128, 129, 1) (27300, 10) (11700, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram for train and test \n",
    "values, count = np.unique(np.argmax(y_train, axis=1), return_counts=True)\n",
    "plt.bar(values, count)\n",
    "\n",
    "values, count = np.unique(np.argmax(y_test, axis=1), return_counts=True)\n",
    "plt.bar(values, count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTZAN Melspectrogram Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class GTZANGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=64, is_test = False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indexes\n",
    "        signals = self.X[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Apply data augmentation\n",
    "        if not self.is_test:\n",
    "            signals = self.__augment(signals)\n",
    "        return signals, self.y[index*self.batch_size:(index+1)*self.batch_size]\n",
    "    \n",
    "    def __augment(self, signals, hor_flip = 0.5, random_cutout = 0.5):\n",
    "        spectrograms =  []\n",
    "        for s in signals:\n",
    "            signal = copy(s)\n",
    "            \n",
    "            # Perform horizontal flip\n",
    "            if np.random.rand() < hor_flip:\n",
    "                signal = np.flip(signal, 1)\n",
    "\n",
    "            # Perform random cutoout of some frequency/time\n",
    "            if np.random.rand() < random_cutout:\n",
    "                lines = np.random.randint(signal.shape[0], size=3)\n",
    "                cols = np.random.randint(signal.shape[0], size=4)\n",
    "                signal[lines, :, :] = -80 # dB\n",
    "                signal[:, cols, :] = -80 # dB\n",
    "\n",
    "            spectrograms.append(signal)\n",
    "        return np.array(spectrograms)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        np.random.shuffle(self.indexes)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN (Melspectrogram version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, n_filters, pool_size=(2, 2)):\n",
    "    x = Conv2D(n_filters, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=pool_size, strides=pool_size)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "def create_model(input_shape, num_genres):\n",
    "    inpt = Input(shape=input_shape)\n",
    "    layer1=inpt#Input(shape=input_shape)\n",
    "    x = conv_block(inpt, 16)\n",
    "    x = conv_block(x, 32)\n",
    "    x = conv_block(x, 64)\n",
    "    x = conv_block(x, 128)\n",
    "    x = conv_block(x, 256)\n",
    "    \n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    \n",
    "    pool_lstm1 = MaxPooling2D((2,2), name = 'pool_lstm')(layer1)\n",
    "    \n",
    "    # Embedding layer\n",
    "\n",
    "    squeezed = Lambda(lambda y: K.squeeze(y, axis= -1))(pool_lstm1)\n",
    "\n",
    "    lstm = Bidirectional(GRU(64))(squeezed) \n",
    "    \n",
    "    \n",
    "    concat = concatenate([x, lstm], axis=-1, name ='concat')    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Dense(512, activation='relu', \n",
    "              kernel_regularizer=tf.keras.regularizers.l2(0.02))(concat)\n",
    "    x = Dropout(0.25)(x)\n",
    "    predictions = Dense(num_genres, \n",
    "                        activation='softmax', \n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "    \n",
    "    model = Model(inputs=inpt, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(X_train[0].shape, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 128, 129, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 129, 16) 160         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 128, 129, 16) 0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 64, 64, 16)   0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 64, 64, 16)   0           max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 32)   4640        dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 32)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 32, 32, 32)   0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 32, 32, 32)   0           max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 64)   18496       dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 32, 32, 64)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 16, 16, 64)   0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 16, 16, 64)   0           max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 128)  73856       dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 128)  0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 8, 8, 128)    0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 8, 8, 128)    0           max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 256)    295168      dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 256)    0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 4, 4, 256)    0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 4, 4, 256)    0           max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool_lstm (MaxPooling2D)        (None, 64, 64, 1)    0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4096)         0           dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 64, 64)       0           pool_lstm[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 4096)         0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128)          49920       lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 4224)         0           dropout_32[0][0]                 \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          2163200     concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 512)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           5130        dropout_33[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,610,570\n",
      "Trainable params: 2,610,570\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceLROnPlat = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.95,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "batch_size = 128\n",
    "train_generator = GTZANGenerator(X_train, y_train)\n",
    "steps_per_epoch = np.ceil(len(X_train)/batch_size)\n",
    "\n",
    "validation_generator = GTZANGenerator(X_test, y_test)\n",
    "val_steps = np.ceil(len(X_test)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "214/214 [==============================] - 167s 779ms/step - loss: 7.3111 - accuracy: 0.1840 - val_loss: 2.7526 - val_accuracy: 0.2626\n",
      "Epoch 2/150\n",
      "214/214 [==============================] - 173s 810ms/step - loss: 2.3154 - accuracy: 0.2844 - val_loss: 2.1028 - val_accuracy: 0.3344\n",
      "Epoch 3/150\n",
      "214/214 [==============================] - 184s 862ms/step - loss: 2.0609 - accuracy: 0.3106 - val_loss: 2.0570 - val_accuracy: 0.3436\n",
      "Epoch 4/150\n",
      "214/214 [==============================] - 193s 900ms/step - loss: 1.9459 - accuracy: 0.3397 - val_loss: 2.0042 - val_accuracy: 0.3387\n",
      "Epoch 5/150\n",
      "214/214 [==============================] - 217s 1s/step - loss: 1.9196 - accuracy: 0.3522 - val_loss: 1.9441 - val_accuracy: 0.3398\n",
      "Epoch 6/150\n",
      "214/214 [==============================] - 182s 851ms/step - loss: 1.9125 - accuracy: 0.3539 - val_loss: 1.9321 - val_accuracy: 0.3606\n",
      "Epoch 7/150\n",
      "214/214 [==============================] - 181s 844ms/step - loss: 1.9009 - accuracy: 0.3703 - val_loss: 1.9084 - val_accuracy: 0.3860\n",
      "Epoch 8/150\n",
      "214/214 [==============================] - 171s 797ms/step - loss: 1.8590 - accuracy: 0.3713 - val_loss: 1.9187 - val_accuracy: 0.3988\n",
      "Epoch 9/150\n",
      "214/214 [==============================] - 167s 782ms/step - loss: 1.8491 - accuracy: 0.3810 - val_loss: 1.8807 - val_accuracy: 0.3956\n",
      "Epoch 10/150\n",
      "214/214 [==============================] - 177s 826ms/step - loss: 1.8143 - accuracy: 0.4006 - val_loss: 1.8108 - val_accuracy: 0.4169\n",
      "Epoch 11/150\n",
      "214/214 [==============================] - 162s 756ms/step - loss: 1.7772 - accuracy: 0.4249 - val_loss: 1.8704 - val_accuracy: 0.3894\n",
      "Epoch 12/150\n",
      "214/214 [==============================] - 161s 754ms/step - loss: 1.8324 - accuracy: 0.3898 - val_loss: 1.7706 - val_accuracy: 0.4419\n",
      "Epoch 13/150\n",
      "214/214 [==============================] - 161s 752ms/step - loss: 1.8096 - accuracy: 0.3981 - val_loss: 1.8064 - val_accuracy: 0.4183\n",
      "Epoch 14/150\n",
      "214/214 [==============================] - 161s 753ms/step - loss: 1.7327 - accuracy: 0.4310 - val_loss: 1.8025 - val_accuracy: 0.4139\n",
      "Epoch 15/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.7205 - accuracy: 0.4355\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "214/214 [==============================] - 162s 758ms/step - loss: 1.7265 - accuracy: 0.4340 - val_loss: 1.8184 - val_accuracy: 0.4210\n",
      "Epoch 16/150\n",
      "214/214 [==============================] - 163s 763ms/step - loss: 1.7345 - accuracy: 0.4300 - val_loss: 1.8089 - val_accuracy: 0.3852\n",
      "Epoch 17/150\n",
      "214/214 [==============================] - 164s 766ms/step - loss: 1.6940 - accuracy: 0.4443 - val_loss: 1.7509 - val_accuracy: 0.4292\n",
      "Epoch 18/150\n",
      "214/214 [==============================] - 163s 760ms/step - loss: 1.7467 - accuracy: 0.4272 - val_loss: 1.8235 - val_accuracy: 0.3910\n",
      "Epoch 19/150\n",
      "214/214 [==============================] - 163s 759ms/step - loss: 1.7325 - accuracy: 0.4289 - val_loss: 1.7501 - val_accuracy: 0.4603\n",
      "Epoch 20/150\n",
      "214/214 [==============================] - 172s 804ms/step - loss: 1.6570 - accuracy: 0.4670 - val_loss: 1.7451 - val_accuracy: 0.4212\n",
      "Epoch 21/150\n",
      "214/214 [==============================] - 175s 816ms/step - loss: 1.6901 - accuracy: 0.4401 - val_loss: 1.7133 - val_accuracy: 0.4507\n",
      "Epoch 22/150\n",
      "214/214 [==============================] - 169s 792ms/step - loss: 1.6675 - accuracy: 0.4546 - val_loss: 1.7167 - val_accuracy: 0.4579\n",
      "Epoch 23/150\n",
      "214/214 [==============================] - 168s 787ms/step - loss: 1.6631 - accuracy: 0.4669 - val_loss: 1.7643 - val_accuracy: 0.4310\n",
      "Epoch 24/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.6623 - accuracy: 0.4639\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "214/214 [==============================] - 167s 781ms/step - loss: 1.6584 - accuracy: 0.4658 - val_loss: 1.7227 - val_accuracy: 0.4360\n",
      "Epoch 25/150\n",
      "214/214 [==============================] - 173s 807ms/step - loss: 1.6152 - accuracy: 0.4752 - val_loss: 1.7030 - val_accuracy: 0.4414\n",
      "Epoch 26/150\n",
      "214/214 [==============================] - 168s 785ms/step - loss: 1.6443 - accuracy: 0.4682 - val_loss: 1.6917 - val_accuracy: 0.4660\n",
      "Epoch 27/150\n",
      "214/214 [==============================] - 164s 766ms/step - loss: 1.6299 - accuracy: 0.4850 - val_loss: 1.6924 - val_accuracy: 0.4776\n",
      "Epoch 28/150\n",
      "214/214 [==============================] - 163s 762ms/step - loss: 1.6090 - accuracy: 0.4865 - val_loss: 1.7170 - val_accuracy: 0.4519\n",
      "Epoch 29/150\n",
      "214/214 [==============================] - 171s 798ms/step - loss: 1.5915 - accuracy: 0.4969 - val_loss: 1.6668 - val_accuracy: 0.4533\n",
      "Epoch 30/150\n",
      "214/214 [==============================] - 175s 820ms/step - loss: 1.5968 - accuracy: 0.4933 - val_loss: 1.6703 - val_accuracy: 0.4723\n",
      "Epoch 31/150\n",
      "214/214 [==============================] - 172s 803ms/step - loss: 1.6046 - accuracy: 0.4972 - val_loss: 1.8143 - val_accuracy: 0.4149\n",
      "Epoch 32/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.5846 - accuracy: 0.5023\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "214/214 [==============================] - 168s 786ms/step - loss: 1.5863 - accuracy: 0.5026 - val_loss: 1.7129 - val_accuracy: 0.4516\n",
      "Epoch 33/150\n",
      "214/214 [==============================] - 165s 771ms/step - loss: 1.5491 - accuracy: 0.5150 - val_loss: 1.6609 - val_accuracy: 0.4752\n",
      "Epoch 34/150\n",
      "214/214 [==============================] - 164s 767ms/step - loss: 1.6027 - accuracy: 0.4943 - val_loss: 1.6481 - val_accuracy: 0.4728\n",
      "Epoch 35/150\n",
      "214/214 [==============================] - 165s 770ms/step - loss: 1.5508 - accuracy: 0.5104 - val_loss: 1.6780 - val_accuracy: 0.4946\n",
      "Epoch 36/150\n",
      "214/214 [==============================] - 169s 789ms/step - loss: 1.5272 - accuracy: 0.5255 - val_loss: 1.5998 - val_accuracy: 0.5036\n",
      "Epoch 37/150\n",
      "214/214 [==============================] - 173s 808ms/step - loss: 1.5172 - accuracy: 0.5321 - val_loss: 1.6767 - val_accuracy: 0.4640\n",
      "Epoch 38/150\n",
      "214/214 [==============================] - 165s 772ms/step - loss: 1.5788 - accuracy: 0.5099 - val_loss: 1.6178 - val_accuracy: 0.5002\n",
      "Epoch 39/150\n",
      "214/214 [==============================] - 176s 824ms/step - loss: 1.5042 - accuracy: 0.5339 - val_loss: 1.5934 - val_accuracy: 0.5012\n",
      "Epoch 40/150\n",
      "214/214 [==============================] - 174s 812ms/step - loss: 1.5441 - accuracy: 0.5170 - val_loss: 1.6114 - val_accuracy: 0.4738\n",
      "Epoch 41/150\n",
      "214/214 [==============================] - 179s 834ms/step - loss: 1.5103 - accuracy: 0.5317 - val_loss: 1.6512 - val_accuracy: 0.4633\n",
      "Epoch 42/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.4987 - accuracy: 0.5212\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0008145062311086804.\n",
      "214/214 [==============================] - 176s 821ms/step - loss: 1.4970 - accuracy: 0.5219 - val_loss: 1.6229 - val_accuracy: 0.5063\n",
      "Epoch 43/150\n",
      "214/214 [==============================] - 168s 785ms/step - loss: 1.5263 - accuracy: 0.5320 - val_loss: 1.5492 - val_accuracy: 0.5231\n",
      "Epoch 44/150\n",
      "214/214 [==============================] - 174s 814ms/step - loss: 1.4162 - accuracy: 0.5729 - val_loss: 1.5693 - val_accuracy: 0.5272\n",
      "Epoch 45/150\n",
      "214/214 [==============================] - 172s 802ms/step - loss: 1.4851 - accuracy: 0.5538 - val_loss: 1.6443 - val_accuracy: 0.4910\n",
      "Epoch 46/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.5106 - accuracy: 0.5300\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0007737808919046074.\n",
      "214/214 [==============================] - 176s 823ms/step - loss: 1.5085 - accuracy: 0.5309 - val_loss: 1.5940 - val_accuracy: 0.5051\n",
      "Epoch 47/150\n",
      "214/214 [==============================] - 175s 820ms/step - loss: 1.4664 - accuracy: 0.5503 - val_loss: 1.5536 - val_accuracy: 0.5309\n",
      "Epoch 48/150\n",
      "214/214 [==============================] - 176s 822ms/step - loss: 1.4954 - accuracy: 0.5369 - val_loss: 1.5415 - val_accuracy: 0.5158\n",
      "Epoch 49/150\n",
      "214/214 [==============================] - 178s 830ms/step - loss: 1.4322 - accuracy: 0.5588 - val_loss: 1.5330 - val_accuracy: 0.5273\n",
      "Epoch 50/150\n",
      "214/214 [==============================] - 178s 834ms/step - loss: 1.4742 - accuracy: 0.5472 - val_loss: 1.5917 - val_accuracy: 0.5071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/150\n",
      "214/214 [==============================] - 177s 827ms/step - loss: 1.4335 - accuracy: 0.5708 - val_loss: 1.6337 - val_accuracy: 0.5202\n",
      "Epoch 52/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.4288 - accuracy: 0.5701\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.000735091819660738.\n",
      "214/214 [==============================] - 174s 812ms/step - loss: 1.4320 - accuracy: 0.5683 - val_loss: 1.6255 - val_accuracy: 0.5093\n",
      "Epoch 53/150\n",
      "214/214 [==============================] - 174s 812ms/step - loss: 1.4126 - accuracy: 0.5762 - val_loss: 1.5961 - val_accuracy: 0.5065\n",
      "Epoch 54/150\n",
      "214/214 [==============================] - 175s 819ms/step - loss: 1.4581 - accuracy: 0.5600 - val_loss: 1.5490 - val_accuracy: 0.5022\n",
      "Epoch 55/150\n",
      "214/214 [==============================] - 178s 831ms/step - loss: 1.4045 - accuracy: 0.5738 - val_loss: 1.6049 - val_accuracy: 0.5032\n",
      "Epoch 56/150\n",
      "214/214 [==============================] - 170s 796ms/step - loss: 1.3655 - accuracy: 0.6044 - val_loss: 1.4899 - val_accuracy: 0.5338\n",
      "Epoch 57/150\n",
      "214/214 [==============================] - 172s 803ms/step - loss: 1.3795 - accuracy: 0.5945 - val_loss: 1.5315 - val_accuracy: 0.5335\n",
      "Epoch 58/150\n",
      "214/214 [==============================] - 168s 786ms/step - loss: 1.4573 - accuracy: 0.5588 - val_loss: 1.5268 - val_accuracy: 0.5115\n",
      "Epoch 59/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.3767 - accuracy: 0.5950\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0006983372120885178.\n",
      "214/214 [==============================] - 165s 773ms/step - loss: 1.3820 - accuracy: 0.5926 - val_loss: 1.4931 - val_accuracy: 0.5501\n",
      "Epoch 60/150\n",
      "214/214 [==============================] - 164s 769ms/step - loss: 1.3857 - accuracy: 0.5839 - val_loss: 1.4885 - val_accuracy: 0.5513\n",
      "Epoch 61/150\n",
      "214/214 [==============================] - 166s 776ms/step - loss: 1.3429 - accuracy: 0.6067 - val_loss: 1.4385 - val_accuracy: 0.5720\n",
      "Epoch 62/150\n",
      "214/214 [==============================] - 172s 804ms/step - loss: 1.3642 - accuracy: 0.6099 - val_loss: 1.5031 - val_accuracy: 0.5443\n",
      "Epoch 63/150\n",
      "214/214 [==============================] - 171s 798ms/step - loss: 1.3792 - accuracy: 0.5934 - val_loss: 1.4776 - val_accuracy: 0.5472\n",
      "Epoch 64/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.3374 - accuracy: 0.6036\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0006634203542489559.\n",
      "214/214 [==============================] - 174s 814ms/step - loss: 1.3348 - accuracy: 0.6051 - val_loss: 1.4660 - val_accuracy: 0.5484\n",
      "Epoch 65/150\n",
      "214/214 [==============================] - 173s 808ms/step - loss: 1.3637 - accuracy: 0.5941 - val_loss: 1.4641 - val_accuracy: 0.5399\n",
      "Epoch 66/150\n",
      "214/214 [==============================] - 180s 841ms/step - loss: 1.3414 - accuracy: 0.6096 - val_loss: 1.4680 - val_accuracy: 0.5797\n",
      "Epoch 67/150\n",
      "214/214 [==============================] - 197s 920ms/step - loss: 1.3499 - accuracy: 0.6040 - val_loss: 1.4138 - val_accuracy: 0.5795\n",
      "Epoch 68/150\n",
      "214/214 [==============================] - 257s 1s/step - loss: 1.3173 - accuracy: 0.6147 - val_loss: 1.4589 - val_accuracy: 0.5533\n",
      "Epoch 69/150\n",
      "214/214 [==============================] - 234s 1s/step - loss: 1.3413 - accuracy: 0.6134 - val_loss: 1.4895 - val_accuracy: 0.5632\n",
      "Epoch 70/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.2609 - accuracy: 0.6367\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.0006302493420662358.\n",
      "214/214 [==============================] - 182s 851ms/step - loss: 1.2571 - accuracy: 0.6380 - val_loss: 1.4303 - val_accuracy: 0.5793\n",
      "Epoch 71/150\n",
      "214/214 [==============================] - 180s 843ms/step - loss: 1.2812 - accuracy: 0.6322 - val_loss: 1.4519 - val_accuracy: 0.5674\n",
      "Epoch 72/150\n",
      "214/214 [==============================] - 183s 853ms/step - loss: 1.3553 - accuracy: 0.5939 - val_loss: 1.4182 - val_accuracy: 0.5693\n",
      "Epoch 73/150\n",
      "214/214 [==============================] - 173s 810ms/step - loss: 1.2981 - accuracy: 0.6275 - val_loss: 1.5160 - val_accuracy: 0.5773\n",
      "Epoch 74/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.3424 - accuracy: 0.6079\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.0005987368611386045.\n",
      "214/214 [==============================] - 180s 842ms/step - loss: 1.3394 - accuracy: 0.6091 - val_loss: 1.4438 - val_accuracy: 0.5640\n",
      "Epoch 75/150\n",
      "214/214 [==============================] - 175s 818ms/step - loss: 1.2703 - accuracy: 0.6277 - val_loss: 1.4568 - val_accuracy: 0.5654\n",
      "Epoch 76/150\n",
      "214/214 [==============================] - 181s 843ms/step - loss: 1.2571 - accuracy: 0.6425 - val_loss: 1.4591 - val_accuracy: 0.5657\n",
      "Epoch 77/150\n",
      "214/214 [==============================] - 176s 820ms/step - loss: 1.2608 - accuracy: 0.6361 - val_loss: 1.3978 - val_accuracy: 0.5795\n",
      "Epoch 78/150\n",
      "214/214 [==============================] - 170s 795ms/step - loss: 1.2675 - accuracy: 0.6320 - val_loss: 1.3823 - val_accuracy: 0.5902\n",
      "Epoch 79/150\n",
      "214/214 [==============================] - 171s 801ms/step - loss: 1.2963 - accuracy: 0.6204 - val_loss: 1.3661 - val_accuracy: 0.5868\n",
      "Epoch 80/150\n",
      "214/214 [==============================] - 174s 814ms/step - loss: 1.2899 - accuracy: 0.6243 - val_loss: 1.4065 - val_accuracy: 0.5914\n",
      "Epoch 81/150\n",
      "214/214 [==============================] - 346s 2s/step - loss: 1.2261 - accuracy: 0.6523 - val_loss: 1.3978 - val_accuracy: 0.5939\n",
      "Epoch 82/150\n",
      "213/214 [============================>.] - ETA: 0s - loss: 1.2469 - accuracy: 0.6438\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0005688000208465382.\n",
      "214/214 [==============================] - 175s 816ms/step - loss: 1.2475 - accuracy: 0.6444 - val_loss: 1.4088 - val_accuracy: 0.5783\n",
      "Epoch 83/150\n",
      "  6/214 [..............................] - ETA: 2:26 - loss: 1.8499 - accuracy: 0.3828"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c619abd23725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     callbacks=[reduceLROnPlat])\n\u001b[0m",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_MaxPoolGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m       data_format=op.get_attr(\"data_format\"))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/music/venv_python3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool_grad\u001b[0;34m(orig_input, orig_output, grad, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   6114\u001b[0m         \u001b[0;34m\"MaxPoolGrad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6115\u001b[0m         \u001b[0morig_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ksize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6116\u001b[0;31m         padding, \"data_format\", data_format)\n\u001b[0m\u001b[1;32m   6117\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6118\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=val_steps,\n",
    "    epochs=150,\n",
    "    verbose=1,\n",
    "    callbacks=[reduceLROnPlat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss = 1.386 and val_acc = 0.577\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"val_loss = {:.3f} and val_acc = {:.3f}\".format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist.history['accuracy'], label='train')\n",
    "plt.plot(hist.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(model.predict(X_test), axis = 1)\n",
    "y_orig = np.argmax(y_test, axis = 1)\n",
    "cm = confusion_matrix(preds, y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = OrderedDict(sorted(genres.items(), key=lambda t: t[1])).keys()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, keys, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(scores):\n",
    "    values, counts = np.unique(scores,return_counts=True)\n",
    "    ind = np.argmax(counts)\n",
    "    return values[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each sound was divided into 39 segments in our custom function\n",
    "scores_songs = np.split(np.argmax(preds, axis=1), 300)\n",
    "scores_songs = [majority_vote(scores) for scores in scores_songs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same analysis for split\n",
    "label = np.split(np.argmax(y_test, axis=1), 300)\n",
    "label = [majority_vote(l) for l in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"majority voting system (acc) = {:.3f}\".format(accuracy_score(label, scores_songs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the classical approach, we are winning now!\n",
    "\n",
    "We passed from a 78.8% accuracy to **82%**. It is not a great improvement but this is a really simple architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('../models/custom_cnn_2d_cnn_gru.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
